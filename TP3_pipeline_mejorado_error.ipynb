{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20287e3b",
   "metadata": {},
   "source": [
    "\n",
    "# TP3 — Pipeline de Regresión (Precios Inmobiliarios) · Scikit‑learn\n",
    "\n",
    "**Objetivo:** predecir el precio en USD (`prp_pre_dol`) a partir de atributos inmobiliarios (numéricos, categóricos y geográficos).  \n",
    "**Tipo de problema:** regresión.  \n",
    "**Métrica principal:** RMSE (en escala original); se reportan también MAE y R².\n",
    "\n",
    "**Checklist de la consigna:**\n",
    "- ✅ Objetivo predictivo definido (regresión sobre `prp_pre_dol`).\n",
    "- ✅ Partición del dataset (hold‑out 80/20 con opción temporal + CV apropiada).\n",
    "- ✅ Pipeline completo (preprocesamiento, modelado, evaluación) con `ColumnTransformer` y `Pipeline`.\n",
    "- ✅ Comparación de al menos dos modelos (RandomForest y HistGradientBoosting).\n",
    "- ✅ Ajuste de hiperparámetros del mejor modelo con **RandomizedSearchCV** optimizado (n_iter y cv reducidos).\n",
    "- ✅ Métricas adecuadas: RMSE, MAE, R²; diagnóstico de over/underfitting y residuos.\n",
    "- ✅ Explicaciones claras de cada decisión (justificaciones debajo de cada bloque).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796dee9b",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuración e imports\n",
    "\n",
    "> Ajustá `DATA_PATH` al CSV que uses. Este notebook asume que la columna objetivo es `prp_pre_dol`.  \n",
    "> Si tu dataset tiene otra columna objetivo, cambiala en `TARGET_COL`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb18e1dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T14:42:10.912692Z",
     "iopub.status.busy": "2025-10-30T14:42:10.912445Z",
     "iopub.status.idle": "2025-10-30T14:42:13.566532Z",
     "shell.execute_reply": "2025-10-30T14:42:13.565329Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuración principal ---\n",
    "DATA_PATH = \"./include/data/processed/propiedades_clean.csv\"  # <— Cambiá a tu ruta real\n",
    "TARGET_COL = \"prp_pre_dol\"   # <— Columna objetivo\n",
    "\n",
    "# Si tu dataset tiene fecha/tiempo para split temporal, ajustá aquí\n",
    "CANDIDATE_DATE_COLS = [\"fecha_publicacion\", \"fecha\", \"alta\", \"publicado\", \"created_at\"]\n",
    "\n",
    "# Parámetros de búsqueda (rápidos y eficientes)\n",
    "RANDOM_STATE = 42\n",
    "N_JOBS = -1\n",
    "CV_FOLDS_DEFAULT = 3         # 3 folds para reducir tiempo\n",
    "N_ITER_SEARCH = 25           # ensayos aleatorios por modelo\n",
    "\n",
    "import os, math, warnings, csv\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d2857",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Carga de datos y limpieza básica\n",
    "\n",
    "- Detección robusta de separador y encoding (pandas por defecto suele funcionar).\n",
    "- Conversión de numéricos en texto (comas/puntos) cuando haga falta.\n",
    "- Saneo del objetivo (`> 0` y finitos).\n",
    "- **Filtro de negocio opcional**: recortar a ≤ 400k USD si querés concentrarte en segmento medio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480c2de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T14:42:13.569254Z",
     "iopub.status.busy": "2025-10-30T14:42:13.568917Z",
     "iopub.status.idle": "2025-10-30T14:42:13.661165Z",
     "shell.execute_reply": "2025-10-30T14:42:13.660349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separador detectado: ';'\n",
      "Shape final: (5745, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>propiedad_id</th>\n",
       "      <th>tip_desc</th>\n",
       "      <th>prp_dom</th>\n",
       "      <th>loc_desc</th>\n",
       "      <th>pro_desc</th>\n",
       "      <th>con_desc</th>\n",
       "      <th>grupo_tip_desc</th>\n",
       "      <th>prp_pre_dol</th>\n",
       "      <th>prp_pre</th>\n",
       "      <th>banos</th>\n",
       "      <th>...</th>\n",
       "      <th>cochera</th>\n",
       "      <th>sup_total</th>\n",
       "      <th>sup_cubierta</th>\n",
       "      <th>prp_alta</th>\n",
       "      <th>prp_mod</th>\n",
       "      <th>url_ficha_inmoup</th>\n",
       "      <th>prp_lat</th>\n",
       "      <th>prp_lng</th>\n",
       "      <th>fotos_cantidad</th>\n",
       "      <th>cocheras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Casa</td>\n",
       "      <td>Juan Pobre 296</td>\n",
       "      <td>Capital</td>\n",
       "      <td>Mendoza</td>\n",
       "      <td>Venta</td>\n",
       "      <td>casas</td>\n",
       "      <td>249000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>600.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>2025-10-04 12:51:06</td>\n",
       "      <td>2025-10-05 17:28:05</td>\n",
       "      <td>https://inmoup.com.ar/311145-cesar-arturo-cort...</td>\n",
       "      <td>-32.870527</td>\n",
       "      <td>-68.885033</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Casa</td>\n",
       "      <td>Calle Norton</td>\n",
       "      <td>Lujan de Cuyo</td>\n",
       "      <td>Mendoza</td>\n",
       "      <td>Venta</td>\n",
       "      <td>casas</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>243.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2025-05-09 12:12:35</td>\n",
       "      <td>2025-09-15 18:14:05</td>\n",
       "      <td>https://inmoup.com.ar/234915-puebla-propiedade...</td>\n",
       "      <td>-33.039367</td>\n",
       "      <td>-68.887107</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Casa</td>\n",
       "      <td>Mzana j</td>\n",
       "      <td>Guaymallen</td>\n",
       "      <td>Mendoza</td>\n",
       "      <td>Venta</td>\n",
       "      <td>casas</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>220.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2025-09-29 19:55:16</td>\n",
       "      <td>2025-10-04 22:24:39</td>\n",
       "      <td>https://inmoup.com.ar/46628-magnaldi-oscar-chr...</td>\n",
       "      <td>-32.910644</td>\n",
       "      <td>-68.756105</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   propiedad_id tip_desc         prp_dom       loc_desc pro_desc con_desc  \\\n",
       "0             1     Casa  Juan Pobre 296        Capital  Mendoza    Venta   \n",
       "4             5     Casa    Calle Norton  Lujan de Cuyo  Mendoza    Venta   \n",
       "5             6     Casa         Mzana j     Guaymallen  Mendoza    Venta   \n",
       "\n",
       "  grupo_tip_desc  prp_pre_dol  prp_pre  banos  ...  cochera  sup_total  \\\n",
       "0          casas     249000.0      0.0    3.0  ...        1      600.0   \n",
       "4          casas     130000.0      0.0    2.0  ...        1      243.0   \n",
       "5          casas      80000.0      0.0    1.0  ...        1      220.0   \n",
       "\n",
       "   sup_cubierta             prp_alta              prp_mod  \\\n",
       "0         210.0  2025-10-04 12:51:06  2025-10-05 17:28:05   \n",
       "4         183.0  2025-05-09 12:12:35  2025-09-15 18:14:05   \n",
       "5          59.0  2025-09-29 19:55:16  2025-10-04 22:24:39   \n",
       "\n",
       "                                    url_ficha_inmoup    prp_lat    prp_lng  \\\n",
       "0  https://inmoup.com.ar/311145-cesar-arturo-cort... -32.870527 -68.885033   \n",
       "4  https://inmoup.com.ar/234915-puebla-propiedade... -33.039367 -68.887107   \n",
       "5  https://inmoup.com.ar/46628-magnaldi-oscar-chr... -32.910644 -68.756105   \n",
       "\n",
       "   fotos_cantidad  cocheras  \n",
       "0               6       NaN  \n",
       "4              12       NaN  \n",
       "5               6       NaN  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "NUMERIC_PREFIXES = (\n",
    "    \"sup\",\n",
    "    \"m2\",\n",
    "    \"metros\",\n",
    "    \"amb\",\n",
    "    \"dorm\",\n",
    "    \"ban\",\n",
    "    \"coch\",\n",
    "    \"fotos\",\n",
    "    \"lat\",\n",
    "    \"lng\",\n",
    "    \"long\",\n",
    "    \"precio\",\n",
    "    \"pre_\",\n",
    "    \"prp_pre\",\n",
    "    \"prp_lat\",\n",
    "    \"prp_lng\",\n",
    ")\n",
    "\n",
    "def coerce_numeric(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convierte strings tipo '1.234,56' a float; deja NaN si no se puede.\"\"\"\n",
    "    if series.dtype.kind in \"biufc\":\n",
    "        return series\n",
    "    s = series.astype(str).str.strip()\n",
    "    mask_comma = s.str.contains(\",\", regex=False)\n",
    "    s = s.where(~mask_comma, s.str.replace(\".\", \"\", regex=False))\n",
    "    s = s.str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def detect_separator(path: str, default: str = \",\", sample_rows: int = 5) -> str:\n",
    "    \"\"\"Intenta inferir el separador principal del CSV.\"\"\"\n",
    "    sample = \"\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8-sig\", errors=\"ignore\") as fh:\n",
    "            sample = \"\".join([fh.readline() for _ in range(sample_rows)])\n",
    "    except FileNotFoundError:\n",
    "        return default\n",
    "    except Exception:\n",
    "        sample = \"\"\n",
    "    if sample:\n",
    "        try:\n",
    "            return csv.Sniffer().sniff(sample).delimiter\n",
    "        except Exception:\n",
    "            pass\n",
    "        if sample.count(\";\") > sample.count(\",\"):\n",
    "            return \";\"\n",
    "    return default\n",
    "\n",
    "# Carga\n",
    "assert os.path.exists(DATA_PATH), f\"No se encontró el archivo: {DATA_PATH}\"\n",
    "separator = detect_separator(DATA_PATH)\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH,\n",
    "    sep=separator,\n",
    "    encoding=\"utf-8-sig\",\n",
    "    low_memory=False,\n",
    ")\n",
    "print(f\"Separador detectado: {separator!r}\")\n",
    "\n",
    "# Normaliza columnas numéricas comunes que a veces vienen como string\n",
    "for c in df.columns:\n",
    "    c_low = c.lower()\n",
    "    if any(c_low.startswith(prefix) for prefix in NUMERIC_PREFIXES):\n",
    "        try:\n",
    "            df[c] = coerce_numeric(df[c])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Sanea objetivo\n",
    "assert TARGET_COL in df.columns, f\"No existe la columna objetivo {TARGET_COL} en el dataset.\"\n",
    "y = df[TARGET_COL].copy()\n",
    "mask_valid = np.isfinite(y) & (y > 0)\n",
    "df = df.loc[mask_valid].copy()\n",
    "y = df[TARGET_COL].copy()\n",
    "\n",
    "# Filtro de negocio (opcional): enfocar hasta 400k USD\n",
    "APLICAR_FILTRO_400K = True\n",
    "if APLICAR_FILTRO_400K:\n",
    "    mask_400k = y <= 400_000\n",
    "    df, y = df.loc[mask_400k].copy(), y.loc[mask_400k].copy()\n",
    "\n",
    "print(\"Shape final:\", df.shape)\n",
    "display(df.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e1cf9",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Partición Train/Test (temporal si es posible)\n",
    "\n",
    "- Si hay columna de fecha válida → **split temporal 80/20** (train = pasado, test = más reciente).  \n",
    "- Si no → `train_test_split` aleatorio 80/20.\n",
    "- La **validación cruzada** posterior respeta la temporalidad con `TimeSeriesSplit` si detectamos fecha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b8ff234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T14:42:13.663122Z",
     "iopub.status.busy": "2025-10-30T14:42:13.662893Z",
     "iopub.status.idle": "2025-10-30T14:42:13.673942Z",
     "shell.execute_reply": "2025-10-30T14:42:13.672740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando split aleatorio 80/20 (sin columna temporal válida).\n",
      "(4596, 20) (1149, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Detecta columna de fecha utilizable\n",
    "date_col = None\n",
    "for c in CANDIDATE_DATE_COLS:\n",
    "    if c in df.columns:\n",
    "        try:\n",
    "            _tmp = pd.to_datetime(df[c], errors='coerce')\n",
    "            if _tmp.notna().sum() > 0:\n",
    "                date_col = c\n",
    "                df[c] = _tmp\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Define X, y\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "\n",
    "def temporal_split(X, y, date_col, test_size=0.2):\n",
    "    order = X[date_col].argsort(kind=\"mergesort\")  # estable\n",
    "    n = len(X)\n",
    "    cut = int((1 - test_size) * n)\n",
    "    idx_train, idx_test = order[:cut], order[cut:]\n",
    "    return X.iloc[idx_train], X.iloc[idx_test], y.iloc[idx_train], y.iloc[idx_test]\n",
    "\n",
    "if date_col is not None:\n",
    "    print(f\"Usando split temporal por columna: {date_col}\")\n",
    "    X_train, X_test, y_train, y_test = temporal_split(X, y, date_col, test_size=0.2)\n",
    "    cv = TimeSeriesSplit(n_splits=CV_FOLDS_DEFAULT)\n",
    "else:\n",
    "    print(\"Usando split aleatorio 80/20 (sin columna temporal válida).\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    cv = CV_FOLDS_DEFAULT\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4fbca1",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Ingeniería de Variables + Preprocesamiento\n",
    "\n",
    "**Numéricas**  \n",
    "- Imputación **mediana** (robusta a outliers).  \n",
    "- **Winsorización** (recorte 1–99%) para estabilizar extremos.  \n",
    "- **Estandarización**.\n",
    "\n",
    "**Categóricas**  \n",
    "- Imputación **más frecuente**.  \n",
    "- **One‑Hot Encoding** con `handle_unknown=\"ignore\"`.\n",
    "\n",
    "**Ingeniería 'segura'**  \n",
    "- Logs (`log_sup_*`, `log_fotos`), ratios (`ratio_cubierta`, `ambientes_m2`) y **geo‑clusters (KMeans)** si hay `lat/lng`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb535e99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T14:42:13.677454Z",
     "iopub.status.busy": "2025-10-30T14:42:13.676841Z",
     "iopub.status.idle": "2025-10-30T14:42:13.692506Z",
     "shell.execute_reply": "2025-10-30T14:42:13.691850Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class WinsorizeNumeric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower=0.01, upper=0.99):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        self.perc_ = {}\n",
    "        for c in X.columns:\n",
    "            col = pd.to_numeric(X[c], errors='coerce')\n",
    "            lo = np.nanpercentile(col, self.lower*100)\n",
    "            hi = np.nanpercentile(col, self.upper*100)\n",
    "            self.perc_[c] = (lo, hi)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, 'perc_')\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for i, c in enumerate(X.columns):\n",
    "            lo, hi = self.perc_[c]\n",
    "            col = pd.to_numeric(X[c], errors='coerce')\n",
    "            col = np.where(col < lo, lo, np.where(col > hi, hi, col))\n",
    "            X[c] = col\n",
    "        return X.values\n",
    "\n",
    "class SafeFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.kmeans_ = None\n",
    "        self.geo_cols_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        lat_col = next((c for c in [\"prp_lat\", \"lat\", \"latitude\"] if c in X.columns), None)\n",
    "        lng_col = next((c for c in [\"prp_lng\", \"lng\", \"long\", \"longitude\"] if c in X.columns), None)\n",
    "        self.geo_cols_ = (lat_col, lng_col)\n",
    "        if lat_col and lng_col:\n",
    "            geo = X[[lat_col, lng_col]].astype(float).dropna()\n",
    "            if len(geo) >= 100:\n",
    "                self.kmeans_ = KMeans(n_clusters=8, random_state=42)\n",
    "                self.kmeans_.fit(geo)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in [\"sup_cubierta\", \"sup_total\", \"fotos\"]:\n",
    "            if col in X.columns:\n",
    "                X[f\"log_{col}\"] = np.log1p(pd.to_numeric(X[col], errors='coerce'))\n",
    "\n",
    "        if \"sup_cubierta\" in X.columns and \"sup_total\" in X.columns:\n",
    "            num = pd.to_numeric(X[\"sup_cubierta\"], errors='coerce')\n",
    "            den = pd.to_numeric(X[\"sup_total\"], errors='coerce').replace(0, np.nan)\n",
    "            X[\"ratio_cubierta\"] = (num / den).fillna(0)\n",
    "\n",
    "        amb_cols = [c for c in X.columns if c.lower().startswith((\"amb\", \"dorm\", \"habit\", \"ba\"))]\n",
    "        if \"sup_total\" in X.columns and len(amb_cols) > 0:\n",
    "            den = pd.to_numeric(X[\"sup_total\"], errors='coerce').replace(0, np.nan)\n",
    "            X[\"ambientes_m2\"] = (\n",
    "                pd.DataFrame({c: pd.to_numeric(X[c], errors='coerce') for c in amb_cols}).sum(axis=1) / den\n",
    "            ).fillna(0)\n",
    "\n",
    "        lat_col, lng_col = self.geo_cols_ if self.geo_cols_ else (None, None)\n",
    "        if self.kmeans_ is not None and lat_col and lng_col:\n",
    "            geo = pd.DataFrame({\n",
    "                \"lat\": pd.to_numeric(X[lat_col], errors='coerce'),\n",
    "                \"lng\": pd.to_numeric(X[lng_col], errors='coerce'),\n",
    "            })\n",
    "            cluster = np.full(len(X), -1)\n",
    "            valid = geo[\"lat\"].notna() & geo[\"lng\"].notna()\n",
    "            if valid.sum() > 0:\n",
    "                cluster[valid] = self.kmeans_.predict(geo.loc[valid, [\"lat\", \"lng\"]])\n",
    "            X[\"geo_cluster\"] = cluster.astype(int)\n",
    "\n",
    "        return X\n",
    "\n",
    "num_cols = [c for c in X_train.columns if pd.api.types.is_numeric_dtype(X_train[c])]\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"winsor\", WinsorizeNumeric(0.01, 0.99)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "feat = Pipeline(steps=[\n",
    "    (\"feateng\", SafeFeatureEngineer()),\n",
    "    (\"pre\", pre)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e4033",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Modelos base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ea2793d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T14:42:13.694619Z",
     "iopub.status.busy": "2025-10-30T14:42:13.694384Z",
     "iopub.status.idle": "2025-10-30T14:42:13.699250Z",
     "shell.execute_reply": "2025-10-30T14:42:13.698027Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "ttr_rf = TransformedTargetRegressor(regressor=rf, func=np.log1p, inverse_func=np.expm1)\n",
    "ttr_hgb = TransformedTargetRegressor(regressor=hgb, func=np.log1p, inverse_func=np.expm1)\n",
    "\n",
    "pipe_rf = Pipeline([(\"features\", feat), (\"model\", ttr_rf)])\n",
    "pipe_hgb = Pipeline([(\"features\", feat), (\"model\", ttr_hgb)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c44f6a",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Benchmark con Validación Cruzada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851bf0ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T14:42:13.701349Z",
     "iopub.status.busy": "2025-10-30T14:42:13.701127Z",
     "iopub.status.idle": "2025-10-30T14:42:18.011157Z",
     "shell.execute_reply": "2025-10-30T14:42:18.010521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV -> RandomForestRegressor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 655, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n        cloned_transformer,\n        ^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n        params=step_params,\n        ^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 719, in fit_transform\n    Xt = self._fit(X, y, routed_params)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n        cloned_transformer,\n        ^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n        params=step_params,\n        ^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py\", line 897, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\joaco\\AppData\\Local\\Temp\\ipykernel_65516\\3187502011.py\", line 72, in transform\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\cluster\\_kmeans.py\", line 1085, in predict\n    X = self._check_test_data(X)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\cluster\\_kmeans.py\", line 944, in _check_test_data\n    X = validate_data(\n        self,\n    ...<5 lines>...\n        accept_large_sparse=False,\n    )\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py\", line 2929, in validate_data\n    _check_feature_names(_estimator, X, reset=reset)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py\", line 2787, in _check_feature_names\n    raise ValueError(message)\nValueError: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- lat\n- lng\nFeature names seen at fit time, yet now missing:\n- prp_lat\n- prp_lng\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, pipe \u001b[38;5;129;01min\u001b[39;00m [(\u001b[33m\"\u001b[39m\u001b[33mRandomForestRegressor\u001b[39m\u001b[33m\"\u001b[39m, pipe_rf), (\u001b[33m\"\u001b[39m\u001b[33mHistGradientBoostingRegressor\u001b[39m\u001b[33m\"\u001b[39m, pipe_hgb)]:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCV -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     res.append(\u001b[43mrun_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     33\u001b[39m cv_df = pd.DataFrame(res).sort_values(\u001b[33m\"\u001b[39m\u001b[33mrmse_cv_mean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m display(cv_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mrun_cv\u001b[39m\u001b[34m(name, pipe, X, y, cv)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_cv\u001b[39m(name, pipe, X=X_train, y=y_train, cv=cv):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     cvres = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     out = {\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: name,\n\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrmse_cv_mean\u001b[39m\u001b[33m\"\u001b[39m: -cvres[\u001b[33m\"\u001b[39m\u001b[33mtest_rmse\u001b[39m\u001b[33m\"\u001b[39m].mean(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfit_time_mean\u001b[39m\u001b[33m\"\u001b[39m: cvres[\u001b[33m\"\u001b[39m\u001b[33mfit_time\u001b[39m\u001b[33m\"\u001b[39m].mean(),\n\u001b[32m     25\u001b[39m     }\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_validation.py:419\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    399\u001b[39m results = parallel(\n\u001b[32m    400\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    401\u001b[39m         clone(estimator),\n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    417\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_validation.py:505\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    499\u001b[39m     all_fits_failed_message = (\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     some_fits_failed_message = (\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 655, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n        cloned_transformer,\n        ^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n        params=step_params,\n        ^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 719, in fit_transform\n    Xt = self._fit(X, y, routed_params)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n        cloned_transformer,\n        ^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n        params=step_params,\n        ^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py\", line 897, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\joaco\\AppData\\Local\\Temp\\ipykernel_65516\\3187502011.py\", line 72, in transform\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\cluster\\_kmeans.py\", line 1085, in predict\n    X = self._check_test_data(X)\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\cluster\\_kmeans.py\", line 944, in _check_test_data\n    X = validate_data(\n        self,\n    ...<5 lines>...\n        accept_large_sparse=False,\n    )\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py\", line 2929, in validate_data\n    _check_feature_names(_estimator, X, reset=reset)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joaco\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py\", line 2787, in _check_feature_names\n    raise ValueError(message)\nValueError: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- lat\n- lng\nFeature names seen at fit time, yet now missing:\n- prp_lat\n- prp_lng\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "\n",
    "scorers = {\n",
    "    \"rmse\": make_scorer(lambda yt, yp: -math.sqrt(mean_squared_error(yt, yp)), greater_is_better=False),\n",
    "    \"mae\": make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "    \"r2\": make_scorer(r2_score),\n",
    "}\n",
    "\n",
    "def run_cv(name, pipe, X=X_train, y=y_train, cv=cv):\n",
    "    cvres = cross_validate(\n",
    "        pipe, X, y, scoring=scorers, cv=cv, n_jobs=-1, return_train_score=True\n",
    "    )\n",
    "    out = {\n",
    "        \"model\": name,\n",
    "        \"rmse_cv_mean\": -cvres[\"test_rmse\"].mean(),\n",
    "        \"rmse_cv_std\": cvres[\"test_rmse\"].std(),\n",
    "        \"mae_cv_mean\": -cvres[\"test_mae\"].mean(),\n",
    "        \"r2_cv_mean\": cvres[\"test_r2\"].mean(),\n",
    "        \"rmse_train_mean\": -cvres[\"train_rmse\"].mean(),\n",
    "        \"mae_train_mean\": -cvres[\"train_mae\"].mean(),\n",
    "        \"r2_train_mean\": cvres[\"train_r2\"].mean(),\n",
    "        \"fit_time_mean\": cvres[\"fit_time\"].mean(),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "res = []\n",
    "for name, pipe in [(\"RandomForestRegressor\", pipe_rf), (\"HistGradientBoostingRegressor\", pipe_hgb)]:\n",
    "    print(f\"CV -> {name}\")\n",
    "    res.append(run_cv(name, pipe))\n",
    "\n",
    "cv_df = pd.DataFrame(res).sort_values(\"rmse_cv_mean\")\n",
    "display(cv_df)\n",
    "best_name = cv_df.iloc[0][\"model\"]\n",
    "print(\"Mejor por RMSE-CV:\", best_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f50d2c",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Tuning eficiente con RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeadfe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "if best_name == \"RandomForestRegressor\":\n",
    "    base = pipe_rf\n",
    "    param_distributions = {\n",
    "        'model__regressor__n_estimators': randint(200, 601),\n",
    "        'model__regressor__max_depth': [10, 12, 15, None],\n",
    "        'model__regressor__min_samples_split': randint(2, 11),\n",
    "        'model__regressor__min_samples_leaf': randint(1, 11),\n",
    "        'model__regressor__max_features': uniform(0.3, 0.7),\n",
    "    }\n",
    "else:\n",
    "    base = pipe_hgb\n",
    "    param_distributions = {\n",
    "        'model__regressor__learning_rate': uniform(0.03, 0.20),\n",
    "        'model__regressor__max_depth': [None, 6, 10],\n",
    "        'model__regressor__min_samples_leaf': randint(5, 31),\n",
    "        'model__regressor__l2_regularization': uniform(0.0, 1.0),\n",
    "        'model__regressor__max_bins': [255],\n",
    "    }\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=25,\n",
    "    cv=cv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "def summarize_search(search):\n",
    "    cols = [\"rank_test_score\",\"mean_test_score\",\"std_test_score\",\"mean_fit_time\"]\n",
    "    df = pd.DataFrame(search.cv_results_)[cols + [\"params\"]].sort_values(\"rank_test_score\")\n",
    "    df[\"rmse_cv\"] = -df[\"mean_test_score\"]\n",
    "    return df[[\"rank_test_score\",\"rmse_cv\",\"std_test_score\",\"mean_fit_time\",\"params\"]]\n",
    "\n",
    "top = summarize_search(search).head(10)\n",
    "display(top)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "print(\"Mejores params:\", search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e6849",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Evaluación final en Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66888158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rel_rmse = rmse / y_test.mean()\n",
    "\n",
    "print(f\"RMSE test: {rmse:,.2f}\")\n",
    "print(f\"MAE  test: {mae:,.2f}\")\n",
    "print(f\"R²   test: {r2:,.4f}\")\n",
    "print(f\"RMSE relativo: {100*rel_rmse:,.2f}% del precio medio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56036ab0",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Diagnóstico gráfico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4928fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred, s=12, alpha=0.6)\n",
    "mn, mx = np.min([y_test.min(), y_pred.min()]), np.max([y_test.max(), y_pred.max()])\n",
    "plt.plot([mn, mx], [mn, mx])\n",
    "plt.xlabel(\"Precio real (USD)\")\n",
    "plt.ylabel(\"Predicción (USD)\")\n",
    "plt.title(\"Real vs Predicción\")\n",
    "plt.show()\n",
    "\n",
    "res = y_test - y_pred\n",
    "plt.figure()\n",
    "plt.scatter(y_pred, res, s=12, alpha=0.6)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"Predicción (USD)\")\n",
    "plt.ylabel(\"Residuo (USD)\")\n",
    "plt.title(\"Residuos vs Predicción\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd82883",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Conclusiones (guion)\n",
    "- Objetivo, métrica y tipo de problema.\n",
    "- Partición temporal/aleatoria y CV.\n",
    "- Pipeline (imputaciones, winsor, OHE, escalado, ingeniería; TTR con log).\n",
    "- Comparación RF vs HGB; selección por RMSE-CV.\n",
    "- Tuning con RandomizedSearchCV (n_iter=25, cv=3, n_jobs=-1).\n",
    "- Resultados en test (RMSE/MAE/R² y RMSE relativo).\n",
    "- Diagnóstico de over/underfitting y próximos pasos.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
