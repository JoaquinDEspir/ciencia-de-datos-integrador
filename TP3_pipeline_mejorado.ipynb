{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20287e3b",
   "metadata": {},
   "source": [
    "\n",
    "# TP3 — Pipeline de Regresión (Precios Inmobiliarios) · Scikit‑learn\n",
    "\n",
    "**Objetivo:** predecir el precio en USD (`prp_pre_dol`) a partir de atributos inmobiliarios (numéricos, categóricos y geográficos).  \n",
    "**Tipo de problema:** regresión.  \n",
    "**Métrica principal:** RMSE (en escala original); se reportan también MAE y R².\n",
    "\n",
    "**Checklist de la consigna:**\n",
    "- ✅ Objetivo predictivo definido (regresión sobre `prp_pre_dol`).\n",
    "- ✅ Partición del dataset (hold‑out 80/20 con opción temporal + CV apropiada).\n",
    "- ✅ Pipeline completo (preprocesamiento, modelado, evaluación) con `ColumnTransformer` y `Pipeline`.\n",
    "- ✅ Comparación de al menos dos modelos (RandomForest y HistGradientBoosting).\n",
    "- ✅ Ajuste de hiperparámetros del mejor modelo con **RandomizedSearchCV** optimizado (n_iter y cv reducidos).\n",
    "- ✅ Métricas adecuadas: RMSE, MAE, R²; diagnóstico de over/underfitting y residuos.\n",
    "- ✅ Explicaciones claras de cada decisión (justificaciones debajo de cada bloque).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796dee9b",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuración e imports\n",
    "\n",
    "> Ajustá `DATA_PATH` al CSV que uses. Este notebook asume que la columna objetivo es `prp_pre_dol`.  \n",
    "> Si tu dataset tiene otra columna objetivo, cambiala en `TARGET_COL`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración principal ---\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path.cwd()\n",
    "DATA_PATH = ROOT_DIR / \"include\" / \"data\" / \"processed\" / \"propiedades_clean.csv\"\n",
    "TARGET_COL = \"prp_pre_dol\"   # Columna objetivo\n",
    "\n",
    "# Si tu dataset tiene fecha/tiempo para split temporal, ajustá acá\n",
    "CANDIDATE_DATE_COLS = [\"fecha_publicacion\", \"fecha\", \"alta\", \"publicado\", \"created_at\"]\n",
    "\n",
    "# Parámetros de búsqueda (rápidos y eficientes)\n",
    "RANDOM_STATE = 42\n",
    "N_JOBS = -1\n",
    "CV_FOLDS_DEFAULT = 3         # 3 folds para reducir tiempo\n",
    "N_ITER_SEARCH = 10           # ensayos aleatorios por modelo\n",
    "\n",
    "import os, math, warnings, csv\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d2857",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Carga de datos y limpieza básica\n",
    "\n",
    "- Detección robusta de separador y encoding (pandas por defecto suele funcionar).\n",
    "- Conversión de numéricos en texto (comas/puntos) cuando haga falta.\n",
    "- Saneo del objetivo (`> 0` y finitos).\n",
    "- **Filtro de negocio opcional**: recortar a ≤ 400k USD si querés concentrarte en segmento medio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUMERIC_PREFIXES = (\n",
    "    \"sup\",\n",
    "    \"m2\",\n",
    "    \"metros\",\n",
    "    \"amb\",\n",
    "    \"dorm\",\n",
    "    \"ban\",\n",
    "    \"coch\",\n",
    "    \"fotos\",\n",
    "    \"lat\",\n",
    "    \"lng\",\n",
    "    \"long\",\n",
    "    \"precio\",\n",
    "    \"pre_\",\n",
    "    \"prp_pre\",\n",
    "    \"prp_lat\",\n",
    "    \"prp_lng\",\n",
    ")\n",
    "\n",
    "def coerce_numeric(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convierte strings tipo '1.234,56' a float; deja NaN si no se puede.\"\"\"\n",
    "    if series.dtype.kind in \"biufc\":\n",
    "        return series\n",
    "    s = series.astype(str).str.strip()\n",
    "    mask_comma = s.str.contains(\",\", regex=False)\n",
    "    s = s.where(~mask_comma, s.str.replace(\".\", \"\", regex=False))\n",
    "    s = s.str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def detect_separator(path: str, default: str = \",\", sample_rows: int = 5) -> str:\n",
    "    \"\"\"Intenta inferir el separador principal del CSV.\"\"\"\n",
    "    sample = \"\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8-sig\", errors=\"ignore\") as fh:\n",
    "            sample = \"\".join([fh.readline() for _ in range(sample_rows)])\n",
    "    except FileNotFoundError:\n",
    "        return default\n",
    "    except Exception:\n",
    "        sample = \"\"\n",
    "    if sample:\n",
    "        try:\n",
    "            return csv.Sniffer().sniff(sample).delimiter\n",
    "        except Exception:\n",
    "            pass\n",
    "        if sample.count(\";\") > sample.count(\",\"):\n",
    "            return \";\"\n",
    "    return default\n",
    "\n",
    "# Carga\n",
    "assert os.path.exists(DATA_PATH), f\"No se encontró el archivo: {DATA_PATH}\"\n",
    "separator = detect_separator(DATA_PATH)\n",
    "print(f\"Cargando dataset desde {DATA_PATH}\")\n",
    "read_csv_kwargs = dict(\n",
    "    sep=separator,\n",
    "    encoding=\"utf-8-sig\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\",\n",
    ")\n",
    "df = pd.read_csv(DATA_PATH, **read_csv_kwargs)\n",
    "print(f\"Separador detectado: {separator!r}\")\n",
    "\n",
    "# Normaliza columnas numéricas comunes que a veces vienen como string\n",
    "for c in df.columns:\n",
    "    c_low = c.lower()\n",
    "    if any(c_low.startswith(prefix) for prefix in NUMERIC_PREFIXES):\n",
    "        try:\n",
    "            df[c] = coerce_numeric(df[c])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Sanea objetivo\n",
    "assert TARGET_COL in df.columns, f\"No existe la columna objetivo {TARGET_COL} en el dataset.\"\n",
    "y = df[TARGET_COL].copy()\n",
    "mask_valid = np.isfinite(y) & (y > 0)\n",
    "df = df.loc[mask_valid].copy()\n",
    "y = df[TARGET_COL].copy()\n",
    "\n",
    "# Filtro de negocio (opcional): enfocar hasta 400k USD\n",
    "APLICAR_FILTRO_400K = True\n",
    "if APLICAR_FILTRO_400K:\n",
    "    mask_400k = y <= 400_000\n",
    "    df, y = df.loc[mask_400k].copy(), y.loc[mask_400k].copy()\n",
    "\n",
    "print(\"Shape final:\", df.shape)\n",
    "display(df.head(3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e1cf9",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Partición Train/Test (temporal si es posible)\n",
    "\n",
    "- Si hay columna de fecha válida → **split temporal 80/20** (train = pasado, test = más reciente).  \n",
    "- Si no → `train_test_split` aleatorio 80/20.\n",
    "- La **validación cruzada** posterior respeta la temporalidad con `TimeSeriesSplit` si detectamos fecha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ff234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detecta columna de fecha utilizable\n",
    "date_col = None\n",
    "for c in CANDIDATE_DATE_COLS:\n",
    "    if c in df.columns:\n",
    "        try:\n",
    "            parsed = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        if parsed.notna().sum() == 0:\n",
    "            continue\n",
    "        df[c] = parsed\n",
    "        date_col = c\n",
    "        break\n",
    "\n",
    "# Define X, y\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "\n",
    "def temporal_split(X: pd.DataFrame, y: pd.Series, date_col: str, test_size: float = 0.2):\n",
    "    ordered = X[date_col].sort_values(kind=\"mergesort\")\n",
    "    ordered_idx = ordered.index\n",
    "    n = len(ordered_idx)\n",
    "    if n < 2:\n",
    "        raise ValueError(\"No hay suficientes filas para realizar un split temporal.\")\n",
    "    cut = int(np.floor((1 - test_size) * n))\n",
    "    cut = min(max(1, cut), n - 1)\n",
    "    idx_train = ordered_idx[:cut]\n",
    "    idx_test = ordered_idx[cut:]\n",
    "    return (\n",
    "        X.loc[idx_train],\n",
    "        X.loc[idx_test],\n",
    "        y.loc[idx_train],\n",
    "        y.loc[idx_test],\n",
    "    )\n",
    "\n",
    "if date_col is not None:\n",
    "    print(f\"Usando split temporal por columna: {date_col}\")\n",
    "    X_train, X_test, y_train, y_test = temporal_split(X, y, date_col, test_size=0.2)\n",
    "    X_train = X_train.drop(columns=[date_col])\n",
    "    X_test = X_test.drop(columns=[date_col])\n",
    "    cv = TimeSeriesSplit(n_splits=CV_FOLDS_DEFAULT)\n",
    "else:\n",
    "    print(\"Usando split aleatorio 80/20 (sin columna temporal válida).\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    cv = CV_FOLDS_DEFAULT\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4fbca1",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Ingeniería de Variables + Preprocesamiento\n",
    "\n",
    "**Numéricas**  \n",
    "- Imputación **mediana** (robusta a outliers).  \n",
    "- **Winsorización** (recorte 1–99%) para estabilizar extremos.  \n",
    "- **Estandarización**.\n",
    "\n",
    "**Categóricas**  \n",
    "- Imputación **más frecuente**.  \n",
    "- **One‑Hot Encoding** con `handle_unknown=\"ignore\"`.\n",
    "\n",
    "**Ingeniería 'segura'**  \n",
    "- Logs (`log_sup_*`, `log_fotos`), ratios (`ratio_cubierta`, `ambientes_m2`) y **geo‑clusters (KMeans)** si hay `lat/lng`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb535e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class WinsorizeNumeric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower=0.01, upper=0.99):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        self.perc_ = {}\n",
    "        for c in X.columns:\n",
    "            col = pd.to_numeric(X[c], errors='coerce')\n",
    "            lo = np.nanpercentile(col, self.lower * 100)\n",
    "            hi = np.nanpercentile(col, self.upper * 100)\n",
    "            self.perc_[c] = (lo, hi)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, 'perc_')\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for i, c in enumerate(X.columns):\n",
    "            lo, hi = self.perc_[c]\n",
    "            col = pd.to_numeric(X[c], errors='coerce')\n",
    "            col = np.where(col < lo, lo, np.where(col > hi, hi, col))\n",
    "            X[c] = col\n",
    "        return X.values\n",
    "\n",
    "class SafeFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.kmeans_ = None\n",
    "        self.geo_cols_ = None\n",
    "        self.kmeans_cols_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        lat_col = next((c for c in [\"prp_lat\", \"lat\", \"latitude\"] if c in X.columns), None)\n",
    "        lng_col = next((c for c in [\"prp_lng\", \"lng\", \"long\", \"longitude\"] if c in X.columns), None)\n",
    "        self.geo_cols_ = (lat_col, lng_col)\n",
    "        self.kmeans_ = None\n",
    "        self.kmeans_cols_ = None\n",
    "        if lat_col and lng_col:\n",
    "            geo = (\n",
    "                X[[lat_col, lng_col]]\n",
    "                .apply(pd.to_numeric, errors='coerce')\n",
    "                .dropna()\n",
    "            )\n",
    "            if len(geo) >= 100:\n",
    "                self.kmeans_ = KMeans(n_clusters=8, random_state=42)\n",
    "                self.kmeans_.fit(geo.to_numpy())\n",
    "                self.kmeans_cols_ = [lat_col, lng_col]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in [\"sup_cubierta\", \"sup_total\", \"fotos\"]:\n",
    "            if col in X.columns:\n",
    "                X[f\"log_{col}\"] = np.log1p(pd.to_numeric(X[col], errors='coerce'))\n",
    "\n",
    "        if \"sup_cubierta\" in X.columns and \"sup_total\" in X.columns:\n",
    "            num = pd.to_numeric(X[\"sup_cubierta\"], errors='coerce')\n",
    "            den = pd.to_numeric(X[\"sup_total\"], errors='coerce').replace(0, np.nan)\n",
    "            X[\"ratio_cubierta\"] = (num / den).fillna(0)\n",
    "\n",
    "        amb_cols = [c for c in X.columns if c.lower().startswith((\"amb\", \"dorm\", \"habit\", \"ba\"))]\n",
    "        if \"sup_total\" in X.columns and len(amb_cols) > 0:\n",
    "            den = pd.to_numeric(X[\"sup_total\"], errors='coerce').replace(0, np.nan)\n",
    "            X[\"ambientes_m2\"] = (\n",
    "                pd.DataFrame({c: pd.to_numeric(X[c], errors='coerce') for c in amb_cols}).sum(axis=1) / den\n",
    "            ).fillna(0)\n",
    "\n",
    "        if self.kmeans_ is not None and self.kmeans_cols_:\n",
    "            missing_cols = [c for c in self.kmeans_cols_ if c not in X.columns]\n",
    "            if not missing_cols:\n",
    "                geo = pd.DataFrame({c: pd.to_numeric(X[c], errors='coerce') for c in self.kmeans_cols_})\n",
    "                cluster = np.full(len(X), -1, dtype=int)\n",
    "                lat_col, lng_col = self.kmeans_cols_\n",
    "                valid = geo[lat_col].notna() & geo[lng_col].notna()\n",
    "                if valid.any():\n",
    "                    cluster[valid] = self.kmeans_.predict(geo.loc[valid, self.kmeans_cols_].to_numpy())\n",
    "                X[\"geo_cluster\"] = cluster\n",
    "\n",
    "        return X\n",
    "\n",
    "num_cols = [c for c in X_train.columns if pd.api.types.is_numeric_dtype(X_train[c])]\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"winsor\", WinsorizeNumeric(0.01, 0.99)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "feat = Pipeline(steps=[\n",
    "    (\"feateng\", SafeFeatureEngineer()),\n",
    "    (\"pre\", pre)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e4033",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Modelos base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "ttr_rf = TransformedTargetRegressor(regressor=rf, func=np.log1p, inverse_func=np.expm1)\n",
    "ttr_hgb = TransformedTargetRegressor(regressor=hgb, func=np.log1p, inverse_func=np.expm1)\n",
    "\n",
    "pipe_rf = Pipeline([(\"features\", feat), (\"model\", ttr_rf)])\n",
    "pipe_hgb = Pipeline([(\"features\", feat), (\"model\", ttr_hgb)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c44f6a",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Benchmark con Validación Cruzada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "\n",
    "scorers = {\n",
    "    \"rmse\": make_scorer(lambda yt, yp: -math.sqrt(mean_squared_error(yt, yp)), greater_is_better=False),\n",
    "    \"mae\": make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "    \"r2\": make_scorer(r2_score),\n",
    "}\n",
    "\n",
    "def run_cv(name, pipe, X=X_train, y=y_train, cv=cv):\n",
    "    cvres = cross_validate(\n",
    "        pipe, X, y, scoring=scorers, cv=cv, n_jobs=-1, return_train_score=True\n",
    "    )\n",
    "    out = {\n",
    "        \"model\": name,\n",
    "        \"rmse_cv_mean\": -cvres[\"test_rmse\"].mean(),\n",
    "        \"rmse_cv_std\": cvres[\"test_rmse\"].std(),\n",
    "        \"mae_cv_mean\": -cvres[\"test_mae\"].mean(),\n",
    "        \"r2_cv_mean\": cvres[\"test_r2\"].mean(),\n",
    "        \"rmse_train_mean\": -cvres[\"train_rmse\"].mean(),\n",
    "        \"mae_train_mean\": -cvres[\"train_mae\"].mean(),\n",
    "        \"r2_train_mean\": cvres[\"train_r2\"].mean(),\n",
    "        \"fit_time_mean\": cvres[\"fit_time\"].mean(),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "res = []\n",
    "for name, pipe in [(\"RandomForestRegressor\", pipe_rf), (\"HistGradientBoostingRegressor\", pipe_hgb)]:\n",
    "    print(f\"CV -> {name}\")\n",
    "    res.append(run_cv(name, pipe))\n",
    "\n",
    "cv_df = pd.DataFrame(res).sort_values(\"rmse_cv_mean\")\n",
    "display(cv_df)\n",
    "best_name = cv_df.iloc[0][\"model\"]\n",
    "print(\"Mejor por RMSE-CV:\", best_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f50d2c",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Tuning eficiente con RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeadfe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "if best_name == \"RandomForestRegressor\":\n",
    "    base = pipe_rf\n",
    "    param_distributions = {\n",
    "        'model__regressor__n_estimators': randint(150, 401),\n",
    "        'model__regressor__max_depth': [10, 12, 15, None],\n",
    "        'model__regressor__min_samples_split': randint(2, 11),\n",
    "        'model__regressor__min_samples_leaf': randint(1, 11),\n",
    "        'model__regressor__max_features': uniform(0.3, 0.7),\n",
    "    }\n",
    "else:\n",
    "    base = pipe_hgb\n",
    "    param_distributions = {\n",
    "        'model__regressor__learning_rate': uniform(0.03, 0.20),\n",
    "        'model__regressor__max_depth': [None, 6, 10],\n",
    "        'model__regressor__min_samples_leaf': randint(5, 31),\n",
    "        'model__regressor__l2_regularization': uniform(0.0, 1.0),\n",
    "        'model__regressor__max_bins': [255],\n",
    "    }\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=N_ITER_SEARCH,\n",
    "    cv=cv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "def summarize_search(search):\n",
    "    cols = [\"rank_test_score\",\"mean_test_score\",\"std_test_score\",\"mean_fit_time\"]\n",
    "    df = pd.DataFrame(search.cv_results_)[cols + [\"params\"]].sort_values(\"rank_test_score\")\n",
    "    df[\"rmse_cv\"] = -df[\"mean_test_score\"]\n",
    "    return df[[\"rank_test_score\",\"rmse_cv\",\"std_test_score\",\"mean_fit_time\",\"params\"]]\n",
    "\n",
    "top = summarize_search(search).head(10)\n",
    "display(top)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "print(\"Mejores params:\", search.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e6849",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Evaluación final en Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66888158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rel_rmse = rmse / y_test.mean()\n",
    "\n",
    "print(f\"RMSE test: {rmse:,.2f}\")\n",
    "print(f\"MAE  test: {mae:,.2f}\")\n",
    "print(f\"R²   test: {r2:,.4f}\")\n",
    "print(f\"RMSE relativo: {100*rel_rmse:,.2f}% del precio medio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56036ab0",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Diagnóstico gráfico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4928fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred, s=12, alpha=0.6)\n",
    "mn, mx = np.min([y_test.min(), y_pred.min()]), np.max([y_test.max(), y_pred.max()])\n",
    "plt.plot([mn, mx], [mn, mx])\n",
    "plt.xlabel(\"Precio real (USD)\")\n",
    "plt.ylabel(\"Predicción (USD)\")\n",
    "plt.title(\"Real vs Predicción\")\n",
    "plt.show()\n",
    "\n",
    "res = y_test - y_pred\n",
    "plt.figure()\n",
    "plt.scatter(y_pred, res, s=12, alpha=0.6)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"Predicción (USD)\")\n",
    "plt.ylabel(\"Residuo (USD)\")\n",
    "plt.title(\"Residuos vs Predicción\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd82883",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Conclusiones (guion)\n",
    "- Objetivo, métrica y tipo de problema.\n",
    "- Partición temporal/aleatoria y CV.\n",
    "- Pipeline (imputaciones, winsor, OHE, escalado, ingeniería; TTR con log).\n",
    "- Comparación RF vs HGB; selección por RMSE-CV.\n",
    "- Tuning con RandomizedSearchCV (n_iter=25, cv=3, n_jobs=-1).\n",
    "- Resultados en test (RMSE/MAE/R² y RMSE relativo).\n",
    "- Diagnóstico de over/underfitting y próximos pasos.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}